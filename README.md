# Simple LLM Training with GPT-2 Architecture

This repository demonstrates how to train a Language Learning Model (LLM) from scratch using the GPT-2 architecture. The model is trained on numerical sequences to learn and predict patterns.

## ğŸ“Œ Overview

This project implements a full machine learning pipeline:

- ğŸ“Š **Synthetic dataset generation** (number sequences)
- ğŸ”¤ **Custom tokenizer training**
- ğŸ§  **Model training** using GPT-2
- ğŸ¤– **Inference capabilities**

---

## ğŸš§ Progress So Far

âœ… We have trained a **6.4 million parameter** model that:
- Uses **base-16 (hexadecimal)** conversion for tokenization.
- Can **add up to 4-digit numbers with 100% accuracy**.
- Is publicly available on Hugging Face:  
  ğŸ”— [mirajnair/simple-llm-gpt2-v1.0](https://huggingface.co/mirajnair/simple-llm-gpt2-v1.0)

---

## ğŸ—ï¸ Dataset Generator

Synthetic number sequences are generated based on parameters defined in `data_config.yaml`.

**Example Configuration:**
- **Number range:** `0 - 9999`
- **Number of sequences:** `100,000`
- **Output path:** `../simple-llm-data/sequences.txt`
- **Delimiters:** `|` (columns), `\n` (rows)

### ğŸ”§ To Generate the Dataset:
1. Update `data_config.yaml` with your desired parameters.
2. Run the generator:
   ```bash
   python3 data_generator.py
   ```

---

## ğŸ¯ Training

### Step 1: Train the Tokenizer
```bash
python3 tokenizer.py
```

### Step 2: Train the Model
```bash
python3 trainer.py
```

Training configurations are managed in `train_config.yaml`, including:

- ğŸ”§ Model architecture (layers, heads, embedding size)
- âš™ï¸ Training hyperparameters (batch size, learning rate)
- ğŸ’¾ Checkpointing and saving
- â˜ï¸ Hugging Face Hub integration

---

## ğŸ”¢ Position Embeddings

### ğŸ“ Learnable vs. Sinusoidal Embeddings

- **Learnable Embeddings**: Adapt to numeric patterns.
- **Sinusoidal Embeddings**: Provide a mathematical basis for position understanding.

---

### ğŸ§® Block Position IDs (Abacus Embedding)

Inspired by the [Abacus Embedding paper](https://arxiv.org/pdf/2405.17399), we use **block position IDs**.

**Example:**

- Input:     `+1342+879+2221+`
- Block IDs: `012340123012340`

#### ğŸ” Why Block Position IDs?

1. âœ… **Commutative Support**: `a + b = b + a` â€” block IDs reinforce this.
2. ğŸ§  **Digit Alignment**: Helps align units, tens, hundreds positions for easier digit-wise processing.

---

### ğŸ”„ Digit Reversal

As part of preprocessing:
- `5672 â†’ 2765` (reversed)
- Output is reversed back during evaluation.

#### ğŸ“ˆ Benefits of Reversal

1. ğŸ§’ **Human-like learning**: Mimics the left-to-right addition humans use.
2. ğŸ¯ **Causal attention compatibility**: Enables better carryover handling.

---

## ğŸ§© Tokenization Strategy

Tokenization is **critical** for arithmetic modeling. Our approach:

1. ğŸ“ **Shortens sequences**: Optimizes context window usage.
2. ğŸ§¬ **Boosts generalization**: Learns across number patterns.
3. ğŸ”„ **Uses base conversion** (e.g., decimal â†’ hexadecimal) for compact, arithmetic-aware tokens.
4. ğŸ§  **Preserves arithmetic logic**: Even in higher bases, rules still apply.

_Weâ€™re experimenting with different bases to improve efficiency further._

---

## ğŸ” Multi-token Prediction

Predicting **multiple tokens at once** increases efficiency.

### Example:

```
Input (reversed):     +12+873+993+PPPP      (P = padding tokens)
Output (reversed):    PPPPPP993+PPPPPP      (P = padding tokens)
Position IDs:         0120123012300000
```

We're currently supporting **2-token prediction** and working on expanding this method.
```


## ğŸ“Š Attention Visualization

Visualizing attention patterns reveals how the model processes arithmetic operations. Below is an example showing attention patterns for the addition problem: `101 + 1002 = 1103` (represented in reversed form as `+101+:2001+:3011+`).

### Layer 1 Attention Patterns

![Layer 1 Attention Visualization](attention_visualizations/layer_1_attention.png)

In this visualization:
- **Bright vertical bars** at positions 1, 5, and 10 show how the model focuses on unit digits from both inputs and the output
- The model learns to align corresponding digit positions (units with units, tens with tens, etc.)
- Attention patterns reveal how information flows during the addition process, including carry operations

This confirms our block position ID approach helps the model understand the commutative nature of addition and properly align digits for arithmetic operations.

The visualization demonstrates how the model has learned to focus on relevant digits when performing calculations, similar to how humans process arithmetic problems.

