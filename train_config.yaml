# Sample train_config.yaml:

paths:
  model_save_path: "../simple-llm-gpt2" # Path to save the model
  model_load_path: "../simple-llm-gpt2" # Path to load the model
  tokenizer_save_path: "../simple-llm-gpt2" # Path to save the tokenizer
  tokenizer_file: "tokenizer.json" # Tokenizer file name

tokenizer:
  vocab_size: 1000  # Or desired vocabulary size

model:
  n_positions: 128      # Maximum sequence length
  n_embd: 256          # Embedding dimension
  n_layer: 6           # Number of transformer layers
  n_head: 8            # Number of attention heads
  activation_function: "gelu" # Activation function
  resid_pdrop: 0.1     # Dropout probability for residual layers
  embd_pdrop: 0.1      # Dropout probability for embeddings
  attn_pdrop: 0.1      # Dropout probability for attention
  layer_norm_epsilon: 0.00001 # Layer norm epsilon
  bos_token_id : None # Beginning of sequence token id
  eos_token_id : None # End of sequence token id

training:
  load_checkpoint: null # Load checkpoint path "../simple-llm-gpt2"
  batch_size: 32 # Batch size
  learning_rate: 0.00005 # Learning rate
  num_epochs: 100 # Number of epochs
  device: "cuda" # Use "cuda" for GPU, "cpu" for CPU
  warmup_steps: 100 # Warmup steps
  gradient_accumulation_steps: 1  # Gradient accumulation steps
  max_grad_norm: 1.0 # Maximum gradient norm
  weight_decay: 0.01 # Weight decay
  eval_interval: 1 # Evaluation interval and save interval
  eval_batch_size: 32 # Evaluation batch size  
  early_stopping: 5 # Early stopping patience
  upload_to_huggingface: true # Upload to Hugging Face Hub
