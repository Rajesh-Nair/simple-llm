# Sample train_config.yaml:

paths:
  model_save_path: "../simple-llm-gpt2-v2.0" # Path to save the model
  model_load_path: "../simple-llm-gpt2-v2.0" # Path to load the model
  tokenizer_save_path: "../simple-llm-gpt2-v2.0" # Path to save the tokenizer
  tokenizer_file: "tokenizer.json" # Tokenizer file name

pre_processing:
  replace_column_delimiter : "+" # None
  reverse_series : True  # False
  column_delimiter : " " # "|"
  sub_seq_add : False # False
  base : 2
  max_length: 32

tokenizer:
  vocab_size: 1  # Or desired vocabulary size
  special_tokens:
    - "<|UNK|>"
    - "<|pad|>"
  ##min_frequency: 2
  #continuing_subword_prefix: "##"

model:
  n_positions: 32      # Maximum sequence length
  n_embd: 192          # Embedding dimension
  n_layer: 6           # Number of transformer layers
  n_head: 6            # Number of attention heads
  activation_function: "gelu" # Activation function
  resid_pdrop: 0.1     # Dropout probability for residual layers
  embd_pdrop: 0.1      # Dropout probability for embeddings
  attn_pdrop: 0.1      # Dropout probability for attention
  layer_norm_epsilon: 0.00001 # Layer norm epsilon
  bos_token_id : None # Beginning of sequence token id
  eos_token_id : None # End of sequence token id

Embedding :
  embedding_type: "fixed" # "fixed" sinusoidal embedding or "block_fixed" sinusoidal embedding or "block" learnable embedding. None for simply trainable embedding
  fixed_pos_theta: 10000.0 # Only used for fixed embedding
  fixed_pos_scaling: 0.1 # Only used for fixed embedding
  fixed_pos_ntk_alpha: 1.0 # Only used for fixed embedding
  block_digit_ids: [3,4] # Only applicable for block_fixed embedding or block embedding
  padding_digit_id: 1 

training:
  load_checkpoint: https://huggingface.co/mirajnair/simple-llm-gpt2-v2.0 # Load checkpoint path "../simple-llm-gpt2-v2.0" or null or huggingface model repo path (https://huggingface.co/mirajnair/simple-llm-gpt2-v2.0)
  per_device_batch_size: 1024 # Batch size
  learning_rate: 0.0001 # Learning rate
  num_epochs: 150 # Number of epochs
  device: "cuda" # Use "cuda" for GPU, "cpu" for CPU
  num_workers: 1 # Number of workers - single node
  warmup_steps: 500 # Warmup steps
  gradient_accumulation_steps: 2  # Gradient accumulation steps
  max_grad_norm: 1.0 # Maximum gradient norm
  weight_decay: 0.01 # Weight decay
  eval_interval: 1 # Evaluation interval and save interval
  per_device_eval_batch_size: 1024 # Evaluation batch size  
  early_stopping: 5 # Early stopping patience
  upload_to_huggingface: True # Upload to Hugging Face Hub
  generate_text_steps : null # Generate text steps
  generate_text_length : 512 # Generate text length
  generate_text_input : "1 1 2 3 5 8 " # Generate text input

wandb:
  enabled: True # Enable wandb
  project_name: "simple-add1" # Wandb project name
  entity: "mirajnair" # Wandb entity


