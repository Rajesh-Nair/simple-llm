# Sample train_config.yaml:

paths:
  model_save_path: "../simple-llm-gpt2-v1.0" # Path to save the model
  model_load_path: "../simple-llm-gpt2-v1.0" # Path to load the model
  tokenizer_save_path: "../simple-llm-gpt2-v1.0" # Path to save the tokenizer
  tokenizer_file: "tokenizer.json" # Tokenizer file name

pre_processing:
  token_delimiter_type : "+" # "chain_of_thought"


tokenizer:
  vocab_size: 11  # Or desired vocabulary size
  special_tokens:
    - "<|UNK|>"
    - "<|pad|>"
  ##min_frequency: 2
  #continuing_subword_prefix: "##"

model:
  n_positions: 512      # Maximum sequence length
  n_embd: 64          # Embedding dimension
  n_layer: 4           # Number of transformer layers
  n_head: 4            # Number of attention heads
  activation_function: "gelu" # Activation function
  resid_pdrop: 0.1     # Dropout probability for residual layers
  embd_pdrop: 0.1      # Dropout probability for embeddings
  attn_pdrop: 0.1      # Dropout probability for attention
  layer_norm_epsilon: 0.00001 # Layer norm epsilon
  bos_token_id : None # Beginning of sequence token id
  eos_token_id : None # End of sequence token id

training:
  load_checkpoint: https://huggingface.co/mirajnair/simple-llm-gpt2-v1.0 # Load checkpoint path "../simple-llm-gpt2-v1.0" or null or huggingface model repo path (https://huggingface.co/mirajnair/simple-llm-gpt2-v1.0)
  per_device_batch_size: 256 # Batch size
  learning_rate: 0.00005 # Learning rate
  num_epochs: 100 # Number of epochs
  device: "cuda" # Use "cuda" for GPU, "cpu" for CPU
  num_workers: 1 # Number of workers - single node
  warmup_steps: 100 # Warmup steps
  gradient_accumulation_steps: 1  # Gradient accumulation steps
  max_grad_norm: 1.0 # Maximum gradient norm
  weight_decay: 0.01 # Weight decay
  eval_interval: 1 # Evaluation interval and save interval
  per_device_eval_batch_size: 256 # Evaluation batch size  
  early_stopping: 5 # Early stopping patience
  upload_to_huggingface: false # Upload to Hugging Face Hub
  generate_text_steps : 5 # Generate text steps
  generate_text_length : 100 # Generate text length
  generate_text_input : "1 1 2 3 5 8 " # Generate text input

