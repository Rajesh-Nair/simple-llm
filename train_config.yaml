# Sample train_config.yaml:

paths:
  model_save_path: "../simple-llm-gpt2-v2.0" # Path to save the model
  model_load_path: "../simple-llm-gpt2-v2.0" # Path to load the model
  tokenizer_save_path: "../simple-llm-gpt2-v2.0" # Path to save the tokenizer
  tokenizer_file: "tokenizer.json" # Tokenizer file name

pre_processing:
  replace_column_delimiter : "+" # None
  reverse_series : True  # False
  column_delimiter : " " # "|"
  sub_seq_add : False # False
  base : 16
  max_length: 64

tokenizer:
  vocab_size: 1  # Or desired vocabulary size
  special_tokens:
    - "<|UNK|>"
    - "<|pad|>"
  ##min_frequency: 2
  #continuing_subword_prefix: "##"

model:
  n_positions: 16      # Maximum sequence length
  n_embd: 256          # Embedding dimension
  n_layer: 8           # Number of transformer layers
  n_head: 4            # Number of attention heads
  activation_function: "gelu" # Activation function
  resid_pdrop: 0.1     # Dropout probability for residual layers
  embd_pdrop: 0.1      # Dropout probability for embeddings
  attn_pdrop: 0.1      # Dropout probability for attention
  layer_norm_epsilon: 0.00001 # Layer norm epsilon
  bos_token_id : None # Beginning of sequence token id
  eos_token_id : None # End of sequence token id
  embedding :
    embedding_type: "block" # "fixed" sinusoidal embedding or "block_fixed" sinusoidal embedding or "block" learnable embedding. None for simply trainable embedding
    fixed_pos_theta: 10000.0 # Only used for fixed embedding
    fixed_pos_scaling: 0.1 # Only used for fixed embedding
    fixed_pos_ntk_alpha: 1.0 # Only used for fixed embedding
    block_digit_ids: [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18] # Only applicable for block_fixed embedding or block embedding
    padding_digit_id: 1 # Applicable for all embedding type
    data_offset: 2 # Applicable for block embedding only

training:
  load_checkpoint: https://huggingface.co/mirajnair/simple-llm-gpt2-v2.0 # Load checkpoint path "../simple-llm-gpt2-v2.0" or null or huggingface model repo path (https://huggingface.co/mirajnair/simple-llm-gpt2-v2.0)
  per_device_batch_size: 1024 # Batch size
  learning_rate: 0.00015 # Learning rate
  num_epochs: 50 # Number of epochs
  device: "cuda" # Use "cuda" for GPU, "cpu" for CPU
  num_workers: 2 # Number of workers - single node
  warmup_steps: 500 # Warmup steps
  gradient_accumulation_steps: 2  # Gradient accumulation steps
  max_grad_norm: 1.0 # Maximum gradient norm
  weight_decay: 0.01 # Weight decay
  eval_interval: 1 # Evaluation interval and save interval
  per_device_eval_batch_size: 1024 # Evaluation batch size  
  early_stopping: 5 # Early stopping patience
  upload_to_huggingface: True # Upload to Hugging Face Hub

wandb:
  enabled: True # Enable wandb
  project_name: "simple-add1" # Wandb project name
  entity: "mirajnair" # Wandb entity


